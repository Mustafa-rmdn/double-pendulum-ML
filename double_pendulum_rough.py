# -*- coding: utf-8 -*-
"""Copy of new_Double_pendulum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-DQ2GudzQi699HWG3C9eCbsedjX9Zx5x
"""

from google.colab import drive
drive.mount('/content/drive')

! pip install -r /content/drive/MyDrive/Github_projects/Double_pendulum/requirement.txt

! pip install numpy sympy pandas scipy matplotlib lightning

import numpy as np
import sympy as sp
import pandas as pd
import scipy.integrate as si
import matplotlib.pyplot as plt
import csv

# defining all the variables and function in sympy
L_1, L_2, m_1, m_2, t, g, k  = sp.symbols(r'L_1, L_2, m_1, m_2, t, g, k')
θ_1, θ_2, r_1, r_2 = sp.symbols(r'\theta_1,\theta_2, r_1, r_2',cls = sp.Function)

x_1, y_1, x_2, y_2 = sp.symbols(r'x_1,y_1,x_2,y_2',cls = sp.Function)
'''
(x_1, y_1) corresponds to the first pendulum whereas (x_2, y_2) referes to the second pendulum.

L1 and L2 refers to the length of the spring when it is in rest. r_1 and r_2 refers to the
extended length because of the stress and strain of spring 1 and 2.

'''


θ_1 = θ_1(t)
θ_2 = θ_2(t)
r_1 = r_1(t)
r_2 = r_2(t)

x_1 = x_1(L_1,θ_1,r_1)
y_1 = y_1(L_1,θ_1,r_1)

x_2 = x_2(L_1,θ_1,r_1,L_2,θ_2,r_2)
y_2 = y_2(L_1,θ_1,r_1,L_2,θ_2,r_2)


# defining the position based on length, stress and the angle of the bob
x_1 = (L_1+r_1)*sp.sin(θ_1)
y_1 = -(L_1+r_1)*sp.cos(θ_1)

x_2 = x_1 + (L_2+r_2)*sp.sin(θ_2)
y_2 = y_1 - (L_2+r_2)*sp.cos(θ_2)


# defining the derivatives with respect to time

x_1_d = sp.diff(x_1,t)
y_1_d = sp.diff(y_1,t)

x_2_d = sp.diff(x_2,t)
y_2_d = sp.diff(y_2,t)

θ_1_d = sp.diff(θ_1,t)
θ_2_d = sp.diff(θ_2,t)

θ_1_dd = sp.diff(θ_1_d,t)
θ_2_dd = sp.diff(θ_2_d,t)

r_1_d = sp.diff(r_1,t)
r_2_d = sp.diff(r_2,t)

r_1_dd = sp.diff(r_1_d,t)
r_2_dd = sp.diff(r_2_d,t)



# Kinetic Energy
T = sp.Rational(1,2)*m_1*(x_1_d**2+y_1_d**2) + sp.Rational(1,2)*m_2*(x_2_d**2+y_2_d**2)


# potential Energy for the system
V = m_1*g*y_1 + m_2*g*y_2 + sp.Rational(1,2)* k*(r_1**2) + sp.Rational(1,2)* k*(r_2**2)



# Lagragian of the system
L = T - V



LE1 = sp.diff(L,θ_1) - sp.diff(sp.diff(L,θ_1_d), t)
LE2 = sp.diff(L,θ_2) - sp.diff(sp.diff(L,θ_2_d), t)

LE3 = sp.diff(L,r_1) - sp.diff(sp.diff(L,r_1_d), t)
LE4 = sp.diff(L,r_2) - sp.diff(sp.diff(L,r_2_d), t)



# solving the lagragian equation

solve = sp.solve([LE1,LE2,LE3,LE4],[θ_1_dd,θ_2_dd,r_1_dd,r_2_dd])


# converting to functions so that we can use these as numerically for further purpose

θ_1_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k],solve[θ_1_dd])
θ_2_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k],solve[θ_2_dd])

r_1_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k],solve[r_1_dd])
r_2_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k],solve[r_2_dd])


# Defining the values for the constant

L_1,L_2,m_1,m_2,g,k = 3,3,1,1,9.8,1


# creating a derivative function to solve the equation

def dSdt(S, t):
    θ_1, ω_1, θ_2, ω_2, r_1, v_1, r_2, v_2 = S[0],S[1],S[2],S[3],S[4],S[5],S[6],S[7]

    solution = np.array([
        ω_1,
        θ_1_dd_n(θ_1,θ_2,r_1,r_2,ω_1,ω_2,v_1,v_2,L_1,L_2,m_1,m_2,g,k),
        ω_2,
        θ_2_dd_n(θ_1,θ_2,r_1,r_2,ω_1,ω_2,v_1,v_2,L_1,L_2,m_1,m_2,g,k),
        v_1,
        r_1_dd_n(θ_1,θ_2,r_1,r_2,ω_1,ω_2,v_1,v_2,L_1,L_2,m_1,m_2,g,k),
        v_2,
        r_2_dd_n(θ_1,θ_2,r_1,r_2,ω_1,ω_2,v_1,v_2,L_1,L_2,m_1,m_2,g,k)
    ])
    return solution

# initial condition

θ_1_0, ω_1_0, θ_2_0, ω_2_0, r_1_0, v_1_0, r_2_0, v_2_0  = 0, 0, np.pi/6, 0, 0.3, 1, 0.2, 1

# The initial conditions
S_0 = np.array([θ_1_0, ω_1_0, θ_2_0, ω_2_0, r_1_0, v_1_0, r_2_0, v_2_0])

time = np.linspace(0, 60, 2000)

# solving the system using odeint function
sol = si.odeint(dSdt, y0=S_0, t=time)


# defining all the solutions to their variables
θ_1, ω_1, θ_2, ω_2, r_1, v_1, r_2, v_2 = sol[:,0],sol[:,1],sol[:,2],sol[:,3],sol[:,4],sol[:,5],sol[:,6],sol[:,7]


# retrieve all the numerical value for x_1, y_1, x_2, y_2 from their predefined equations

def retrieve_value(θ_1, θ_2, r_1, r_2):
    return np.array([(L_1+r_1)*np.sin(θ_1),
                    -(L_1+r_1)*np.cos(θ_1),
                     (L_1+r_1)*np.sin(θ_1) + (L_2+r_2)*np.sin(θ_2),
                     -(L_1+r_1)*np.cos(θ_1) - (L_2+r_2)*np.cos(θ_2)])

x_1, y_1, x_2, y_2 = retrieve_value(θ_1, θ_2, r_1, r_2)

"""## Saving more frames per simulation
it helps the model to learn about the dynamics and the effect of parameters on the trajectories.
"""

# defining all the variables and function in sympy
L_1, L_2, m_1, m_2, t, g, k  = sp.symbols(r'L_1, L_2, m_1, m_2, t, g, k')
θ_1, θ_2, r_1, r_2 = sp.symbols(r'\theta_1,\theta_2, r_1, r_2',cls = sp.Function)

x_1, y_1, x_2, y_2 = sp.symbols(r'x_1,y_1,x_2,y_2',cls = sp.Function)
'''
(x_1, y_1) corresponds to the first pendulum whereas (x_2, y_2) referes to the second pendulum.

L1 and L2 refers to the length of the spring when it is in rest. r_1 and r_2 refers to the
extended length because of the stress and strain of spring 1 and 2.

'''


θ_1 = θ_1(t)
θ_2 = θ_2(t)
r_1 = r_1(t)
r_2 = r_2(t)

x_1 = x_1(L_1,θ_1,r_1)
y_1 = y_1(L_1,θ_1,r_1)

x_2 = x_2(L_1,θ_1,r_1,L_2,θ_2,r_2)
y_2 = y_2(L_1,θ_1,r_1,L_2,θ_2,r_2)


# defining the position based on length, stress and the angle of the bob
x_1 = (L_1+r_1)*sp.sin(θ_1)
y_1 = -(L_1+r_1)*sp.cos(θ_1)

x_2 = x_1 + (L_2+r_2)*sp.sin(θ_2)
y_2 = y_1 - (L_2+r_2)*sp.cos(θ_2)


# defining the derivatives with respect to time

x_1_d = sp.diff(x_1,t)
y_1_d = sp.diff(y_1,t)

x_2_d = sp.diff(x_2,t)
y_2_d = sp.diff(y_2,t)

θ_1_d = sp.diff(θ_1,t)
θ_2_d = sp.diff(θ_2,t)

θ_1_dd = sp.diff(θ_1_d,t)
θ_2_dd = sp.diff(θ_2_d,t)

r_1_d = sp.diff(r_1,t)
r_2_d = sp.diff(r_2,t)

r_1_dd = sp.diff(r_1_d,t)
r_2_dd = sp.diff(r_2_d,t)



# Kinetic Energy
T = sp.Rational(1,2)*m_1*(x_1_d**2+y_1_d**2) + sp.Rational(1,2)*m_2*(x_2_d**2+y_2_d**2)


# potential Energy for the system
V = m_1*g*y_1 + m_2*g*y_2 + sp.Rational(1,2)* k*(r_1**2) + sp.Rational(1,2)* k*(r_2**2)



# Lagragian of the system
L = T - V



LE1 = sp.diff(L,θ_1) - sp.diff(sp.diff(L,θ_1_d), t)
LE2 = sp.diff(L,θ_2) - sp.diff(sp.diff(L,θ_2_d), t)

LE3 = sp.diff(L,r_1) - sp.diff(sp.diff(L,r_1_d), t)
LE4 = sp.diff(L,r_2) - sp.diff(sp.diff(L,r_2_d), t)



# solving the lagragian equation

solve = sp.solve([LE1,LE2,LE3,LE4],[θ_1_dd,θ_2_dd,r_1_dd,r_2_dd])


# converting to functions so that we can use these as numerically for further purpose

θ_1_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k],solve[θ_1_dd])
θ_2_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k],solve[θ_2_dd])

r_1_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k],solve[r_1_dd])
r_2_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k],solve[r_2_dd])


# Defining the values for the constant

# L_1,L_2,m_1,m_2,g,k = 3,3,1,1,9.8,1


# creating a derivative function to solve the equation

def dSdt(S, t):
    θ_1, ω_1, θ_2, ω_2, r_1, v_1, r_2, v_2 = S[0],S[1],S[2],S[3],S[4],S[5],S[6],S[7]

    solution = np.array([
        ω_1,
        θ_1_dd_n(θ_1,θ_2,r_1,r_2,ω_1,ω_2,v_1,v_2,L_1,L_2,m_1,m_2,g,k),
        ω_2,
        θ_2_dd_n(θ_1,θ_2,r_1,r_2,ω_1,ω_2,v_1,v_2,L_1,L_2,m_1,m_2,g,k),
        v_1,
        r_1_dd_n(θ_1,θ_2,r_1,r_2,ω_1,ω_2,v_1,v_2,L_1,L_2,m_1,m_2,g,k),
        v_2,
        r_2_dd_n(θ_1,θ_2,r_1,r_2,ω_1,ω_2,v_1,v_2,L_1,L_2,m_1,m_2,g,k)
    ])
    return solution



def retrieve_value(θ_1, θ_2, r_1, r_2):
    return np.array([(L_1+r_1)*np.sin(θ_1),
                    -(L_1+r_1)*np.cos(θ_1),
                     (L_1+r_1)*np.sin(θ_1) + (L_2+r_2)*np.sin(θ_2),
                     -(L_1+r_1)*np.cos(θ_1) - (L_2+r_2)*np.cos(θ_2)])

time = np.linspace(0, 60, 2000)


# --------------------------------------------------------------------------------
# 2) Loop through runs, each time with different (random or varied) initial conditions
# --------------------------------------------------------------------------------

save_folder = "double_pendulum_figs_multiple_frames"
os.makedirs(save_folder, exist_ok=True)

csv_filename = "initial_conditions.csv"

time_series_folder = "time_series"
os.makedirs(time_series_folder, exist_ok=True)




n_runs = 150
time_array = np.linspace(0, 180, 2000)

# We'll store the initial conditions in a list so we can write them to CSV afterward
run_data = []  # list of [run_index, θ1_0, ω1_0, θ2_0, ω2_0, r1_0, v1_0, r2_0, v2_0]

ts_index_rows = []

for i in range(n_runs):
    # Example: random initial conditions (customize ranges as desired)

# L_1,L_2,m_1,m_2,g,k = 3,3,1,1,9.8,1
    L_1 = np.random.uniform(1,3)
    L_2 = np.random.uniform(1,5)
    m_1 = np.random.uniform(0.5,1.5)
    m_2 = np.random.uniform(0.5,1.5)
    g = 9.8
    k = np.random.uniform(0.8,1.5)


    θ1_0 = np.random.uniform(-np.pi/2, np.pi/2)
    ω1_0 = np.random.uniform(-1, 1)
    θ2_0 = np.random.uniform(-np.pi/2, np.pi/2)
    ω2_0 = np.random.uniform(-1, 1)
    r1_0 = np.random.uniform(0, 0.5)
    v1_0 = np.random.uniform(-2, 2)
    r2_0 = np.random.uniform(0, 0.5)
    v2_0 = np.random.uniform(-2, 2)


    # Create initial state vector
    S0 = np.array([θ1_0, ω1_0, θ2_0, ω2_0, r1_0, v1_0, r2_0, v2_0])

    # Solve ODE
    sol = si.odeint(dSdt, S0, time_array)

    # Unpack solution
    θ1_arr, ω1_arr = sol[:,0], sol[:,1]
    θ2_arr, ω2_arr = sol[:,2], sol[:,3]
    r1_arr, v1_arr = sol[:,4], sol[:,5]
    r2_arr, v2_arr = sol[:,6], sol[:,7]

    # for time series analysis
    ## storing coordinates for each simulation run

    x1_time_series_traj, y1_time_series_traj, x2_time_series_traj, y2_time_series_traj = retrieve_value(θ1_arr, θ2_arr, r1_arr, r2_arr)

    # Stack into a (T, 4) array
    coords = np.stack([x1_time_series_traj, y1_time_series_traj, x2_time_series_traj, y2_time_series_traj], axis=1)    # shape: [T, 4]

    ts_path = os.path.join(time_series_folder, f"run_{i+1:04d}.npz")

    np.savez_compressed(

    ts_path,

    t=time_array.astype(np.float32),   # [T]

    coords=coords.astype(np.float32),    # [T,4]
    theta_1 = θ1_arr.astype(np.float32),
    omega_1 = ω1_ar.astype(np.float32),
    theta_2 = θ2_arr.astype(np.float32),
    omega_2 = ω2_arr.astype(np.float32),
    r_1 = r1_arr.astype(np.float32),
    v_1 = v1_arr.astype(np.float32),
    r_2 = r2_arr.astype(np.float32),
    v_2 = v2_arr.astype(np.float32)
    )

    ts_index_rows.append([ts_path, i+1, L_1, L_2, m_1, m_2, k])





    # Build trajectory for plotting
    x1_traj, y1_traj = [], []
    x2_traj, y2_traj = [], []
    for idx in range(len(time_array)):
        x1_, y1_, x2_, y2_ = retrieve_value(θ1_arr[idx], θ2_arr[idx],
                                           r1_arr[idx], r2_arr[idx])
        x1_traj.append(x1_)
        y1_traj.append(y1_)
        x2_traj.append(x2_)
        y2_traj.append(y2_)


        if len(x2_traj) % SAVE_EVERY == 0:
                # --- constants for consistent rasterization across the whole dataset ---
            SIDE = 256            # final image size in pixels (square)
            DPI = 128             # SIDE = DPI * figsize_in_inches
            FIGSIZE = (SIDE / DPI, SIDE / DPI)

            # Fixed world coordinates to avoid scale leakage
            # With L1∈[1,3], L2∈[1,5], small r, ±10 covers the full swing with margin
            WORLD_XY = 10.0

            # How often to snapshot within a run (use windows, not cumulative if you like)
            SAVE_EVERY = 100

        # inside your loop after accumulating trajectories...

            fig = plt.figure(figsize=FIGSIZE, dpi=DPI)
            ax = plt.gca()

            # OPTIONAL: choose a random window (prevents start-phase bias).
            # Comment this block out if you want cumulative paths instead.
            W = SAVE_EVERY                 # window length in samples
            if len(x2_traj) >= W:
                import random
                s = random.randint(0, len(x2_traj) - W)
                x1_seg = x1_traj[s:s+W] if 'x1_traj' in locals() else None
                y1_seg = y1_traj[s:s+W] if 'y1_traj' in locals() else None
                x2_seg = x2_traj[s:s+W]
                y2_seg = y2_traj[s:s+W]
            else:
                x1_seg, y1_seg = (x1_traj, y1_traj) if 'x1_traj' in locals() else (None, None)
                x2_seg, y2_seg = x2_traj, y2_traj

            # Plot both bobs if available (better identifiability), with thicker, anti-aliased lines
            if x1_seg is not None and y1_seg is not None:
                ax.plot(x1_seg, y1_seg, linewidth=2.0, solid_capstyle='round', antialiased=True)
            ax.plot(x2_seg, y2_seg, linewidth=2.0, solid_capstyle='round', antialiased=True)

            # Fix world scale and remove any text/ticks
            ax.set_xlim(-WORLD_XY, WORLD_XY)
            ax.set_ylim(-WORLD_XY, WORLD_XY)
            ax.set_aspect('equal', adjustable='box')
            ax.set_axis_off()
            plt.margins(0)

            # Ensure canvas fills the figure (no dynamic cropping)
            ax.set_position([0, 0, 1, 1])

            # Save with fixed dpi and no 'tight' cropping
            frame_idx = len(x2_traj) // SAVE_EVERY
            fig_filename = os.path.join(save_folder, f"double_pendulum_run_{i+1}_frame_{frame_idx}.png")
            fig.savefig(fig_filename, dpi=DPI, bbox_inches=None, pad_inches=0)
            plt.close(fig)

            print(f"[Run {i+1}] Figure saved -> {fig_filename}")

    # Use the actual saved path; keep frame as integer
    run_data.append([
        fig_filename, i+1, frame_idx,
        L_1, L_2, m_1, m_2, k,
        θ1_0, ω1_0, θ2_0, ω2_0, r1_0, v1_0, r2_0, v2_0
    ])


# --------------------------------------------------------------------------------
# 3) Save the initial conditions to a CSV file
# --------------------------------------------------------------------------------
# Write the run data to a CSV file

with open(csv_filename, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["Path_name", "Run", "Frame",
                     "L_1","L_2","m_1","m_2","k",
                     "θ1_0","ω1_0","θ2_0","ω2_0","r1_0","v_1","r_2","v_2"])
    writer.writerows(run_data)
print(f"Initial conditions saved to {csv_filename}")


time_series_header = [
    "ts_path",   # file path to the saved .npz
    "run_id",    # which simulation (1..n_runs)
    "L_1", "L_2",
    "m_1", "m_2",
    "k"          # length of time_array (number of timesteps)
]

with open("timeseries_index.csv", "w", newline="") as f:
    wr = csv.writer(f)
    wr.writerow(time_series_header)            # write column names once
    wr.writerows(ts_index_rows)    # write all rows you collected

"""## Saving more frames per simulation (faster)
it helps the model to learn about the dynamics and the effect of parameters on the trajectories.
"""

import os
import csv
import numpy as np
import sympy as sp
import matplotlib.pyplot as plt
from scipy import integrate as si

# ==============================
# 1) Sympy setup (from your code)
# ==============================
L_1, L_2, m_1, m_2, t, g, k  = sp.symbols(r'L_1, L_2, m_1, m_2, t, g, k')
θ_1, θ_2, r_1, r_2 = sp.symbols(r'\theta_1,\theta_2, r_1, r_2', cls=sp.Function)

x_1, y_1, x_2, y_2 = sp.symbols(r'x_1,y_1,x_2,y_2', cls=sp.Function)

# Make them time-dependent
θ_1 = θ_1(t); θ_2 = θ_2(t)
r_1 = r_1(t); r_2 = r_2(t)

# Define kinematics
x_1 = (L_1 + r_1)*sp.sin(θ_1)
y_1 = -(L_1 + r_1)*sp.cos(θ_1)

x_2 = x_1 + (L_2 + r_2)*sp.sin(θ_2)
y_2 = y_1 - (L_2 + r_2)*sp.cos(θ_2)

# Derivatives
x_1_d = sp.diff(x_1, t); y_1_d = sp.diff(y_1, t)
x_2_d = sp.diff(x_2, t); y_2_d = sp.diff(y_2, t)

θ_1_d = sp.diff(θ_1, t); θ_2_d = sp.diff(θ_2, t)
θ_1_dd = sp.diff(θ_1_d, t); θ_2_dd = sp.diff(θ_2_d, t)

r_1_d = sp.diff(r_1, t); r_2_d = sp.diff(r_2, t)
r_1_dd = sp.diff(r_1_d, t); r_2_dd = sp.diff(r_2_d, t)

# Energies and Lagrangian
T = sp.Rational(1,2)*m_1*(x_1_d**2 + y_1_d**2) + sp.Rational(1,2)*m_2*(x_2_d**2 + y_2_d**2)
V = m_1*g*y_1 + m_2*g*y_2 + sp.Rational(1,2)*k*(r_1**2) + sp.Rational(1,2)*k*(r_2**2)
L = T - V

LE1 = sp.diff(L, θ_1) - sp.diff(sp.diff(L, θ_1_d), t)
LE2 = sp.diff(L, θ_2) - sp.diff(sp.diff(L, θ_2_d), t)
LE3 = sp.diff(L, r_1) - sp.diff(sp.diff(L, r_1_d), t)
LE4 = sp.diff(L, r_2) - sp.diff(sp.diff(L, r_2_d), t)

solve = sp.solve([LE1, LE2, LE3, LE4], [θ_1_dd, θ_2_dd, r_1_dd, r_2_dd])

# Lambdify accelerations
θ_1_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k], solve[θ_1_dd], 'numpy')
θ_2_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k], solve[θ_2_dd], 'numpy')
r_1_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k], solve[r_1_dd], 'numpy')
r_2_dd_n = sp.lambdify([θ_1,θ_2,r_1,r_2,θ_1_d,θ_2_d,r_1_d,r_2_d,L_1,L_2,m_1,m_2,g,k], solve[r_2_dd], 'numpy')

# ODE state derivative
def dSdt(S, t, L1, L2, m1, m2, gval, kval):
    θ1, ω1, θ2, ω2, r1, v1, r2, v2 = S
    return np.array([
        ω1,
        θ_1_dd_n(θ1,θ2,r1,r2,ω1,ω2,v1,v2,L1,L2,m1,m2,gval,kval),
        ω2,
        θ_2_dd_n(θ1,θ2,r1,r2,ω1,ω2,v1,v2,L1,L2,m1,m2,gval,kval),
        v1,
        r_1_dd_n(θ1,θ2,r1,r2,ω1,ω2,v1,v2,L1,L2,m1,m2,gval,kval),
        v2,
        r_2_dd_n(θ1,θ2,r1,r2,ω1,ω2,v1,v2,L1,L2,m1,m2,gval,kval),
    ], dtype=np.float64)

# ==============================
# 2) Config
# ==============================
n_runs = 150
time_array = np.linspace(0, 180, 2000)   # adjust length for speed vs fidelity
gval = 9.8

# Output folders
save_folder = "dataset/images"
time_series_folder = "dataset/timeseries"
index_folder = "dataset/indexes"
os.makedirs(save_folder, exist_ok=True)
os.makedirs(time_series_folder, exist_ok=True)
os.makedirs(index_folder, exist_ok=True)

# Image rendering config (fixed scale to avoid leakage)
SIDE = 256
DPI = 128
FIGSIZE = (SIDE / DPI, SIDE / DPI)
WORLD_XY = 10.0
W = 100                                # window length
K_from_T = lambda T: T // W            # number of windows per run
J = 15                                  # jitter for start indices

# Index buffers
image_index_rows = []
ts_index_rows = []

# ==============================
# 3) Main simulation loop
# ==============================
for i in range(n_runs):
    # Random parameters / ICs
    L1 = float(np.random.uniform(1,3))
    L2 = float(np.random.uniform(1,5))
    m1 = float(np.random.uniform(0.5,1.5))
    m2 = float(np.random.uniform(0.5,1.5))
    kval = float(np.random.uniform(0.8,1.5))

    θ1_0 = float(np.random.uniform(-np.pi/2, np.pi/2))
    ω1_0 = float(np.random.uniform(-1, 1))
    θ2_0 = float(np.random.uniform(-np.pi/2, np.pi/2))
    ω2_0 = float(np.random.uniform(-1, 1))
    r1_0 = float(np.random.uniform(0, 0.5))
    v1_0 = float(np.random.uniform(-2, 2))
    r2_0 = float(np.random.uniform(0, 0.5))
    v2_0 = float(np.random.uniform(-2, 2))

    S0 = np.array([θ1_0, ω1_0, θ2_0, ω2_0, r1_0, v1_0, r2_0, v2_0], dtype=np.float64)

    # Solve ODE
    sol = si.odeint(dSdt, S0, time_array, args=(L1,L2,m1,m2,gval,kval))

    # Unpack
    θ1_arr, ω1_arr = sol[:,0], sol[:,1]
    θ2_arr, ω2_arr = sol[:,2], sol[:,3]
    r1_arr, v1_arr = sol[:,4], sol[:,5]
    r2_arr, v2_arr = sol[:,6], sol[:,7]

    # Vectorized coordinates
    x1 = (L1 + r1_arr) * np.sin(θ1_arr)
    y1 = -(L1 + r1_arr) * np.cos(θ1_arr)
    x2 = x1 + (L2 + r2_arr) * np.sin(θ2_arr)
    y2 = y1 - (L2 + r2_arr) * np.cos(θ2_arr)
    coords = np.stack([x1, y1, x2, y2], axis=1).astype(np.float32)   # [T,4]

    # ---- Save .npz per run ----
    ts_path = os.path.join(time_series_folder, f"run_{i+1:05d}.npz")
    np.savez_compressed(
        ts_path,
        t=time_array.astype(np.float32),
        coords=coords,
        theta_1=θ1_arr.astype(np.float32), theta_2=θ2_arr.astype(np.float32),
        r_1=r1_arr.astype(np.float32), r_2=r2_arr.astype(np.float32),
        ω_1=ω1_arr.astype(np.float32), ω_2=ω2_arr.astype(np.float32),
        v_1=v1_arr.astype(np.float32), v_2=v2_arr.astype(np.float32),
        L_1=np.float32(L1), L_2=np.float32(L2),
        m_1=np.float32(m1), m_2=np.float32(m2),
        k=np.float32(kval), g=np.float32(gval)
    )
    ts_index_rows.append([ts_path, i+1, L1, L2, m1, m2, kval, len(time_array)])

    # ---- Render K windows as images (no inner time loop) ----
    T = len(time_array)
    K = K_from_T(T)
    starts = np.linspace(0, T - W, K).astype(int)
    if T > W:
        jitter = np.random.randint(-J, J+1, size=K)
        starts = np.clip(starts + jitter, 0, T - W)

    for frame_idx, s in enumerate(starts, start=1):
        x1_seg = x1[s:s+W]; y1_seg = y1[s:s+W]
        x2_seg = x2[s:s+W]; y2_seg = y2[s:s+W]

        fig = plt.figure(figsize=FIGSIZE, dpi=DPI)
        ax = plt.gca()
        ax.plot(x1_seg, y1_seg, linewidth=2.0, solid_capstyle='round', antialiased=True)
        ax.plot(x2_seg, y2_seg, linewidth=2.0, solid_capstyle='round', antialiased=True)

        ax.set_xlim(-WORLD_XY, WORLD_XY)
        ax.set_ylim(-WORLD_XY, WORLD_XY)
        ax.set_aspect('equal', adjustable='box')
        ax.set_axis_off()
        ax.set_position([0, 0, 1, 1])  # fill canvas

        fig_filename = os.path.join(save_folder, f"run_{i+1:05d}_frame_{frame_idx:03d}.png")
        fig.savefig(fig_filename, dpi=DPI, bbox_inches=None, pad_inches=0)
        plt.close(fig)


        image_index_rows.append([fig_filename, i+1, frame_idx, L1, L2, m1, m2, kval])
    print(f"[Run {i+1}] Figure saved -> {fig_filename}")
# ==============================
# 4) Write indexes once
# ==============================
with open(os.path.join(index_folder, "timeseries_index.csv"), "w", newline="") as f:
    wr = csv.writer(f)
    wr.writerow(["ts_path","run_id","L_1","L_2","m_1","m_2","k","T"])
    wr.writerows(ts_index_rows)

with open(os.path.join(index_folder, "image_index.csv"), "w", newline="") as f:
    wr = csv.writer(f)
    wr.writerow(["path","run_id","frame_id","L_1","L_2","m_1","m_2","k"])
    wr.writerows(image_index_rows)

print("✓ Done. Saved:")
print("  - Time series NPZ files in:", time_series_folder)
print("  - Images in:", save_folder)
print("  - Indexes in:", index_folder)



import shutil
import os

zip_file_name = "dataset.zip"
zip_file_path = os.path.join("/content", zip_file_name)
dataset_folder_path = "/content/dataset"

shutil.make_archive(zip_file_path.replace(".zip", ""), 'zip', dataset_folder_path)

print(f"Folder compressed to {zip_file_path}")

from google.colab import files

files.download(zip_file_path)

"""# Loading files"""

import torch
from torch.utils.data import DataLoader, random_split
import numpy as np
import os
import pandas as pd



time_series_files = os.listdir("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/timeseries")

# Load all 'coords' tensors into a list
coords_tensors = []
for file_name in time_series_files:
    file_path = os.path.join("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/timeseries", file_name)
    # Load the 'coords' array and convert it to a PyTorch tensor
    tensor = torch.from_numpy(np.load(file_path)['coords'])
    coords_tensors.append(tensor)
coords_tensors = torch.stack(coords_tensors)



coords_tensors.shape

number_of_validation_data, number_of_testing_data = int(0.1 * coords_tensors.shape[0]), int(0.1 * coords_tensors.shape[0])
number_of_training_data = int(coords_tensors.shape[0] - number_of_validation_data - number_of_testing_data)

testing_dataset, validation_dataset, training_dataset = random_split(coords_tensors, [number_of_testing_data, number_of_validation_data, number_of_training_data],torch.Generator().manual_seed(42))

"""# LSTM"""

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
import numpy as np
import os
import pandas as pd
import lightning as L
from torchmetrics.regression import MeanSquaredError, R2Score
import wandb

!wandb login
wandb.init(project="Double_Pendulum", name="LSTM_v2")

time_series_files = os.listdir("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/timeseries")

# Load all 'coords' tensors into a list
coords_tensors = []
for file_name in time_series_files:
    file_path = os.path.join("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/timeseries", file_name)
    # Load the 'coords' array and convert it to a PyTorch tensor
    tensor = torch.from_numpy(np.load(file_path)['coords']).float()
    coords_tensors.append(tensor)
coords_tensors = torch.stack(coords_tensors)

gt_parameters = pd.read_csv("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/indexes/timeseries_index.csv")
gt_parameters_tensor = torch.tensor(gt_parameters.iloc[:, 2:-1].values, dtype=torch.float32)

class LSTM_fc_architecture(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_layers_for_lstm, num_output):
      super().__init__()


      self.input_size = input_size
      self.hidden_size = hidden_size
      self.num_layers = num_layers_for_lstm
      self.num_output = num_output

      self.lstm = torch.nn.LSTM(self.input_size, self.hidden_size, self.num_layers, dropout= 0.5, batch_first= True)  # lstm expect input as (batch, row, column= Batch, sequence_length(time), features)
      self.fc1 = torch.nn.Linear(in_features = hidden_size, out_features = 128)
      self.fc2 = torch.nn.Linear(in_features= 128, out_features= 64)
      self.fc3 = torch.nn.Linear(in_features= 64, out_features= self.num_output)
      self.dropout = torch.nn.Dropout(p=0.5)

    def forward(self,x):
      output_lstm, (h_n, c_n) = self.lstm(x)      # output_lstm (B, T, Hidden)
      x = self.fc1(output_lstm[:, -1, :])         # last time_step (B,Hidden)
      x = F.relu(x)
      x = self.dropout(x)
      x = self.fc2(x)
      x = F.relu(x)
      x = self.dropout(x)
      x = self.fc3(x)
      return x

class Double_pendulum_dataset:
  def __init__(self, coords_tensors,ground_truth_parameters):

    self.coords_tensors = coords_tensors
    self.ground_truth_parameters = ground_truth_parameters

  def __len__(self):
    return self.coords_tensors.shape[0]

  def __getitem__(self, idx):

    coords = self.coords_tensors[idx]
    ground_truth = self.ground_truth_parameters[idx]

    return coords, ground_truth

def collate_fn_detection(batch):
  coords_tensors, ground_truth = list(zip(*batch))
  return list(coords_tensors), list(ground_truth)

full_trajectories = Double_pendulum_dataset(coords_tensors = coords_tensors, ground_truth_parameters= gt_parameters_tensor)

number_of_validation_data, number_of_testing_data = int(0.1 * len(full_trajectories)), int(0.1 * len(full_trajectories))
number_of_training_data = int(len(full_trajectories) - number_of_validation_data - number_of_testing_data)

testing_dataset, validation_dataset, training_dataset = random_split(full_trajectories, [number_of_testing_data, number_of_validation_data, number_of_training_data],torch.Generator().manual_seed(42))


training_dataloader = DataLoader(training_dataset,
                                 batch_size= 10,
                                 shuffle= True,
                                #  collate_fn= collate_fn_detection
                                 )

validation_dataloader = DataLoader(validation_dataset,
                                   batch_size= 10,
                                   shuffle= False,
                                  #  collate_fn= collate_fn_detection
                                   )

testing_dataloader = DataLoader(testing_dataset,
                                batch_size = 10,
                                shuffle= False,
                                # collate_fn= collate_fn_detection
                                )

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
import numpy as np
import os
import pandas as pd
import lightning as L
from torchmetrics.regression import MeanSquaredError, R2Score
import wandb
import torch.nn as nn # Import nn module

class LSTM_fc_lighting(L.LightningModule):
  def __init__(self,learning_rate):
    super().__init__()

    self.save_hyperparameters()
    self.learning_rate = learning_rate

    self.model = LSTM_fc_architecture(input_size= 4, hidden_size= 256, num_layers_for_lstm= 2, num_output= 5)

    self.train_mse = MeanSquaredError()
    self.train_r2 = R2Score()

    self.valid_mse = MeanSquaredError()
    self.valid_r2 = R2Score()

    self.test_mse = MeanSquaredError()
    self.test_r2 = R2Score()

    self.counts = {"train": 0, "validation": 0, "test": 0}




  def forward(self, coords_tensors):

    output = self.model(coords_tensors)
    return output

  def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)

    return optimizer


  def common_step(self, batch, batch_idx,stage):
    coords_tensors, ground_truth = batch
    output = self.model(coords_tensors)

    loss = F.mse_loss(output, ground_truth)


    wandb.log({f"{stage}_loss": loss})
    self.log(f"{stage}_loss",loss,on_epoch=True, on_step=False,logger=True)


    if stage == "train":
        self.train_mse.update(output, ground_truth)
        self.train_r2.update(output, ground_truth)
    elif stage == "validation":
        self.valid_mse.update(output, ground_truth)
        self.valid_r2.update(output, ground_truth)
    else:
        self.test_mse.update(output, ground_truth)
        self.test_r2.update(output, ground_truth)

    # increment by batch size
    self.counts[stage] += 1

    return loss



  def training_step(self, batch, batch_idx):

    loss = self.common_step(batch, batch_idx, "train")


    return loss

  def validation_step(self, batch, batch_idx):

    loss = self.common_step(batch, batch_idx, "validation")

    return loss

  def test_step(self, batch, batch_idx):

    loss = self.common_step(batch, batch_idx, "test")

    return loss


  def on_validation_epoch_end(self):

    valid_mse_result = float(self.valid_mse.compute())
    if self.counts["validation"] >= 2:
      valid_r2_result = float(self.valid_r2.compute())
    else:
      valid_r2_result = 0.0

    wandb.log({f"validation_mse": valid_mse_result})
    wandb.log({f"validation_r2": valid_r2_result})
    self.log("validation_mse", valid_mse_result,logger=True, prog_bar=True)
    self.log("validation_r2",  valid_r2_result,logger=True, prog_bar=True)

    self.valid_mse.reset()
    self.valid_r2.reset()


  def on_test_epoch_end(self):

    test_mse_result = float(self.test_mse.compute())
    if self.counts["test"] >= 2:
      test_r2_result = float(self.test_r2.compute())
    else:
      test_r2_result = 0.0


    wandb.log({f"test_mse": test_mse_result})
    wandb.log({f"test_r2": test_r2_result})
    self.log("test_mse", test_mse_result,logger=True, prog_bar=True)
    self.log("test_r2",  test_r2_result,logger=True, prog_bar=True)

    self.test_mse.reset()
    self.test_r2.reset()

  def on_train_epoch_end(self):

    train_mse_result = float(self.train_mse.compute())
    if self.counts["train"] >= 2:
      train_r2_result = float(self.train_r2.compute())
    else:
      train_r2_result = 0.0

    wandb.log({f"train_mse": train_mse_result})
    wandb.log({f"train_r2": train_r2_result})
    self.log("train_mse", train_mse_result,logger=True, prog_bar=True)
    self.log("train_r2",  train_r2_result,logger=True, prog_bar=True)


    self.train_mse.reset()
    self.train_r2.reset()





# Instantiating the model again after modification
lstm_fc_model = LSTM_fc_lighting(learning_rate= 1e-5)

trainer_for_lstm_fc = L.Trainer(max_epochs= 50,
                    # fast_dev_run=True,
                    # log_every_n_steps = 1,
                    # logger = [csv_logger,tensorboard_logger, wandb_logger],
                    # callbacks=[ckpt, RichProgressBar()],
                    # default_root_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/LSTM/"
                    )
tuner = L.pytorch.tuner.Tuner(trainer_for_lstm_fc)

lr_finder = tuner.lr_find(
    lstm_fc_model,
    train_dataloaders= training_dataloader,
    val_dataloaders= validation_dataloader,
    min_lr=1e-6, max_lr=1e-2, num_training=100  # ~200 steps sweep
)

fig = lr_finder.plot(suggest=True)
new_lr = lr_finder.suggestion()     # Lightning's suggestion
print("Suggested LR:", new_lr)

from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint
from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger, CSVLogger



csv_logger = CSVLogger(save_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/LSTM/")
tensorboard_logger = TensorBoardLogger(save_dir = "/content/drive/MyDrive/Github_projects/Double_pendulum/LSTM/")
wandb_logger = WandbLogger(project="Double_Pendulum", name="LSTM")

ckpt = ModelCheckpoint(
    monitor="validation_mse",
    mode="min",
    save_top_k=1,
    filename="LSTM-{epoch:02d}-{validation_mse:.4f}"
)

trainer_for_lstm_fc = L.Trainer(max_epochs= 100,
                    # fast_dev_run=True,
                    log_every_n_steps = 1,
                    logger = [csv_logger,tensorboard_logger, wandb_logger],
                    callbacks=[ckpt, RichProgressBar()],
                    default_root_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/LSTM/")

trainer_for_lstm_fc.fit(model = lstm_fc_model, train_dataloaders= training_dataloader, val_dataloaders= validation_dataloader)

tester = L.Trainer(
    # logger=[csv_logger, tensorboard_logger, wandb_logger],
    enable_progress_bar=False  # simplest
    # or callbacks=[]  # no RichProgressBar here
)

tester.test(
    model=lstm_fc_model,
    dataloaders=testing_dataloader,
    # ckpt_path="/content/drive/MyDrive/Github_projects/Double_pendulum/LSTM/lightning_logs/version_4/checkpoints/epoch=49-step=600.ckpt",
    ckpt_path = "/content/drive/MyDrive/Github_projects/Double_pendulum/LSTM/lightning_logs/version_0/checkpoints/LSTM-epoch=99-validation_mse=1.5636.ckpt"
)

import torch
import pandas as pd

def show_prediction_and_ground_truth(model, dataloader, n_limit=None):
    """
    Collect predictions and ground truths for all samples in a dataloader.

    model: trained Lightning or torch model
    dataloader: DataLoader yielding (x, y)
    param_names: list of parameter names ["L1","L2","m1","m2","k"]
    n_limit: optional cap on number of samples to include
    """
    model.eval()
    param_names = ["L1","L2","m1","m2","k"]

    preds, gts = [], []
    total = 0

    for x, y in dataloader:
        x = x.to(model.device)
        with torch.no_grad():
            y_hat = model(x)
        preds.append(y_hat.cpu())
        gts.append(y.cpu())

        total += len(y)
        if n_limit and total >= n_limit:
            break

    preds = torch.cat(preds, dim=0).numpy()
    gts = torch.cat(gts, dim=0).numpy()

    df_pred = pd.DataFrame(preds, columns=[f"pred_{n}" for n in param_names])
    df_gt = pd.DataFrame(gts, columns=[f"gt_{n}" for n in param_names])

    df = pd.concat([df_gt, df_pred], axis=1)
    return df
show_prediction_and_ground_truth(model =lstm_fc_model,dataloader = testing_dataloader)

"""# Attention Model"""

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
import numpy as np
import os
import math
import pandas as pd
import lightning as L
from torchmetrics.regression import MeanSquaredError, R2Score
import wandb

!wandb login
wandb.init(project="Double_Pendulum", name="Transformer_v2")

time_series_files = os.listdir("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/timeseries")

# Load all 'coords' tensors into a list
coords_tensors = []
for file_name in time_series_files:
    file_path = os.path.join("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/timeseries", file_name)
    # Load the 'coords' array and convert it to a PyTorch tensor
    tensor = torch.from_numpy(np.load(file_path)['coords']).float()
    coords_tensors.append(tensor)
coords_tensors = torch.stack(coords_tensors)

gt_parameters = pd.read_csv("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/indexes/timeseries_index.csv")
gt_parameters_tensor = torch.tensor(gt_parameters.iloc[:, 2:-1].values, dtype=torch.float32)

class PostionalEncoding(nn.Module):
  def __init__(self,d_model, max_seq_length):
    super().__init__()
    self.d_model = d_model
    self.max_seq_length = max_seq_length

    pe = torch.zeros((self.max_seq_length, self.d_model), dtype= torch.float32)
    position = torch.arange(0,self.max_seq_length, dtype= torch.float32).unsqueeze(1)
    divide_term = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(self.max_seq_length) / d_model)
        )
    pe[:,0::2]= torch.sin(position * divide_term)
    pe[:,1::2]= torch.cos(position * divide_term)

    pe = pe.unsqueeze(0)
    self.register_buffer("pe", pe)

  def forward(self, x):
    B, T, d = x.shape
    return x + self.pe[:,:T,:]



class Transformer_architecture(nn.Module):
  def __init__(self,d_model, n_head, num_layers):
    super().__init__()
    self.d_model = d_model
    self.n_head = n_head
    self.num_layers = num_layers

    self.projection_512 = nn.Linear(in_features = 4, out_features= self.d_model)
    self.encoder_layer = nn.TransformerEncoderLayer(d_model = self.d_model , nhead = self.n_head, batch_first= True)
    self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layer, num_layers= self.num_layers)
    self.positional_encoder = PostionalEncoding(d_model= self.d_model, max_seq_length= 10000)

    self.learnable_layer = nn.Parameter(torch.zeros(1,1,d_model), requires_grad=True)

    self.fc1 = nn.Linear(in_features = self.d_model, out_features= int(self.d_model/2))
    self.fc2 = nn.Linear(in_features = int(self.d_model/2), out_features= int(self.d_model/4))
    self.fc3 = nn.Linear(in_features = int(self.d_model/4), out_features= 5)
    self.dropout = nn.Dropout(p=0.2)

  def forward(self,x):
    x = self.projection_512(x)


    B = x.shape[0]

    learnable_layer = self.learnable_layer.expand(B,-1,-1)

    full_encoder = torch.cat((learnable_layer,x),dim = 1)
    full_encoder = self.positional_encoder(full_encoder)


    encoder_output = self.encoder(full_encoder)

    final_feature = encoder_output[:,0,:]

    x = self.fc1(final_feature)

    x = F.relu(x)
    x = self.dropout(x)
    x = self.fc2(x)
    x = F.relu(x)
    x = self.dropout(x)
    x = self.fc3(x)
    return x

class Double_pendulum_dataset:
  def __init__(self, coords_tensors,ground_truth_parameters):

    self.coords_tensors = coords_tensors
    self.ground_truth_parameters = ground_truth_parameters

  def __len__(self):
    return self.coords_tensors.shape[0]

  def __getitem__(self, idx):

    coords = self.coords_tensors[idx]
    ground_truth = self.ground_truth_parameters[idx]

    return coords, ground_truth

full_trajectories = Double_pendulum_dataset(coords_tensors = coords_tensors, ground_truth_parameters= gt_parameters_tensor)

number_of_validation_data, number_of_testing_data = int(0.1 * len(full_trajectories)), int(0.1 * len(full_trajectories))
number_of_training_data = int(len(full_trajectories) - number_of_validation_data - number_of_testing_data)

testing_dataset, validation_dataset, training_dataset = random_split(full_trajectories, [number_of_testing_data, number_of_validation_data, number_of_training_data],torch.Generator().manual_seed(42))


training_dataloader = DataLoader(training_dataset,
                                 batch_size= 10,
                                 shuffle= True,
                                 )

validation_dataloader = DataLoader(validation_dataset,
                                   batch_size= 10,
                                   shuffle= False,
                                   )

testing_dataloader = DataLoader(testing_dataset,
                                batch_size = 10,
                                shuffle= False,
                                )

class Transformer_lighting(L.LightningModule):
  def __init__(self,learning_rate):
    super().__init__()

    self.save_hyperparameters()
    self.learning_rate = learning_rate

    self.model = Transformer_architecture(d_model= 512, n_head= 8, num_layers= 5)

    self.train_mse = MeanSquaredError()
    self.train_r2 = R2Score()

    self.valid_mse = MeanSquaredError()
    self.valid_r2 = R2Score()

    self.test_mse = MeanSquaredError()
    self.test_r2 = R2Score()

    self.counts = {"train": 0, "validation": 0, "test": 0}



  def forward(self, coords_tensors):
    # coords_tensors = coords_tensors.to_device(self.device)
    output = self.model(coords_tensors)
    return output

  def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)

    return optimizer


  def common_step(self, batch, batch_idx,stage):
    coords_tensors, ground_truth = batch
    output = self.model(coords_tensors)

    loss = F.mse_loss(output, ground_truth)


    wandb.log({f"{stage}_loss": loss})
    self.log(f"{stage}_loss",loss,on_epoch=True, on_step=False,logger=True)

    if stage == "train":
        self.train_mse.update(output, ground_truth)
        self.train_r2.update(output, ground_truth)
    elif stage == "validation":
        self.valid_mse.update(output, ground_truth)
        self.valid_r2.update(output, ground_truth)
    else:
        self.test_mse.update(output, ground_truth)
        self.test_r2.update(output, ground_truth)

    # increment by batch size
    self.counts[stage] += 1


    return loss



  def training_step(self, batch, batch_idx):
    loss = self.common_step(batch, batch_idx, "train")

    return loss

  def validation_step(self, batch, batch_idx):

    loss = self.common_step(batch, batch_idx, "validation")

    return loss

  def test_step(self, batch, batch_idx):

    loss = self.common_step(batch, batch_idx, "test")

    return loss

  def on_validation_epoch_end(self):

    valid_mse_result = float(self.valid_mse.compute())
    if self.counts["validation"] >= 2:
      valid_r2_result = float(self.valid_r2.compute())
    else:
      valid_r2_result = 0.0

    wandb.log({f"validation_mse": valid_mse_result})
    wandb.log({f"validation_r2": valid_r2_result})
    self.log("validation_mse", valid_mse_result,logger=True, prog_bar=True)
    self.log("validation_r2",  valid_r2_result,logger=True, prog_bar=True)

    self.valid_mse.reset()
    self.valid_r2.reset()


  def on_test_epoch_end(self):

    test_mse_result = float(self.test_mse.compute())
    if self.counts["test"] >= 2:
      test_r2_result = float(self.test_r2.compute())
    else:
      test_r2_result = 0.0


    wandb.log({f"test_mse": test_mse_result})
    wandb.log({f"test_r2": test_r2_result})
    self.log("test_mse", test_mse_result,logger=True, prog_bar=True)
    self.log("test_r2",  test_r2_result,logger=True, prog_bar=True)

    self.test_mse.reset()
    self.test_r2.reset()

  def on_train_epoch_end(self):

    train_mse_result = float(self.train_mse.compute())
    if self.counts["train"] >= 2:
      train_r2_result = float(self.train_r2.compute())
    else:
      train_r2_result = 0.0

    wandb.log({f"train_mse": train_mse_result})
    wandb.log({f"train_r2": train_r2_result})
    self.log("train_mse", train_mse_result,logger=True, prog_bar=True)
    self.log("train_r2",  train_r2_result,logger=True, prog_bar=True)


    self.train_mse.reset()
    self.train_r2.reset()




transformer_model = Transformer_lighting(learning_rate= 3e-5)

trainer_for_transformer = L.Trainer(max_epochs= 50,
                    # fast_dev_run=True,
                    # log_every_n_steps = 1,
                    # logger = [csv_logger,tensorboard_logger, wandb_logger],
                    # callbacks=[ckpt, RichProgressBar()],
                    # default_root_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/LSTM/"
                    )
tuner = L.pytorch.tuner.Tuner(trainer_for_transformer)

lr_finder = tuner.lr_find(
    transformer_model,
    train_dataloaders= training_dataloader,
    val_dataloaders= validation_dataloader,
    min_lr=1e-6, max_lr=1e-2, num_training=100  # ~200 steps sweep
)

fig = lr_finder.plot(suggest=True)
new_lr = lr_finder.suggestion()     # Lightning's suggestion
print("Suggested LR:", new_lr)

from lightning.pytorch.callbacks import RichProgressBar
from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger, CSVLogger

tensorboard_logger = TensorBoardLogger(save_dir = "/content/drive/MyDrive/Github_projects/Double_pendulum/Transformer/")
wandb_logger = WandbLogger(project="Double_Pendulum", name="Transformer")


ckpt = ModelCheckpoint(
    monitor="validation_mse",
    mode="min",
    save_top_k=1,
    filename="Attention-{epoch:02d}-{validation_mse:.4f}"
)


csv_logger = CSVLogger(save_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/Transformer/")
trainer_for_transformer = L.Trainer(max_epochs= 200,
                    # fast_dev_run=True,
                    log_every_n_steps = 1,
                    logger = [csv_logger, tensorboard_logger, wandb_logger],
                    accelerator="auto",
                    callbacks=[ckpt, RichProgressBar()],
                    default_root_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/Transformer/")

trainer_for_transformer.fit(model = transformer_model, train_dataloaders= training_dataloader, val_dataloaders= validation_dataloader)

tester = L.Trainer(
    # logger=[csv_logger, tensorboard_logger, wandb_logger],
    enable_progress_bar=False  # simplest
    # or callbacks=[]  # no RichProgressBar here
)

tester.test(
    model=transformer_model,
    dataloaders=testing_dataloader,
    ckpt_path = "/content/drive/MyDrive/Github_projects/Double_pendulum/Transformer/lightning_logs/version_0/checkpoints/Attention-epoch=152-validation_mse=0.1290.ckpt"
)

import torch
import pandas as pd

def show_prediction_and_ground_truth(model, dataloader, n_limit=None):
    """
    Collect predictions and ground truths for all samples in a dataloader.

    model: trained Lightning or torch model
    dataloader: DataLoader yielding (x, y)
    param_names: list of parameter names ["L1","L2","m1","m2","k"]
    n_limit: optional cap on number of samples to include
    """
    model.eval()
    param_names = ["L1","L2","m1","m2","k"]

    preds, gts = [], []
    total = 0

    for x, y in dataloader:
        x = x.to(model.device)
        with torch.no_grad():
            y_hat = model(x)
        preds.append(y_hat.cpu())
        gts.append(y.cpu())

        total += len(y)
        if n_limit and total >= n_limit:
            break

    preds = torch.cat(preds, dim=0).numpy()
    gts = torch.cat(gts, dim=0).numpy()

    df_pred = pd.DataFrame(preds, columns=[f"pred_{n}" for n in param_names])
    df_gt = pd.DataFrame(gts, columns=[f"gt_{n}" for n in param_names])

    df = pd.concat([df_gt, df_pred], axis=1)
    return df
show_prediction_and_ground_truth(model =transformer_model,dataloader = testing_dataloader)

"""# CNN"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
import numpy as np
import os
import math
import pandas as pd
import lightning as L
import matplotlib.pyplot as plt
from torchmetrics.regression import MeanSquaredError, R2Score
from torchvision.io import read_image
import wandb

!wandb login
wandb.init(project="Double_Pendulum", name="CNN_v2")

image_directory = "/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/images/"

image_files = os.listdir(image_directory)
image_files = np.sort(image_files)
image_paths = [os.path.join(image_directory, f) for f in image_files]

image_tensor = torch.stack([read_image(im).float() for im in image_paths])  # [N, C, H, W]


gt_parameters = pd.read_csv("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/indexes/image_index.csv")
gt_parameters_tensor = torch.tensor(gt_parameters.iloc[:,3:].values, dtype=torch.float32)

class CNN_architecture(nn.Module):
  def __init__(self):
    super().__init__()

    self.conv_layer_1 = nn.Conv2d(in_channels= 4, out_channels= 32, kernel_size=3, stride=1, padding=1)
    self.conv_layer_2 = nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size=3, stride=1, padding=1)
    self.max_pool = nn.MaxPool2d(kernel_size= 2, stride=2)
    self.relu = nn.ReLU()

    self.adapt_pool = nn.AdaptiveAvgPool2d((64, 64))
    self.flatten = nn.Flatten()


    self.fc = nn.Sequential(
        nn.Linear(in_features= 64*64*64, out_features= 512),
        nn.ReLU(),
        nn.Dropout(p=0.2),
        nn.Linear(in_features= 512, out_features= 256),
        nn.ReLU(),
        nn.Dropout(p=0.2),
        nn.Linear(in_features=256, out_features= 128),
        nn.ReLU(),
        nn.Dropout(p=0.2),
        nn.Linear(in_features= 128, out_features= 5)
    )

  def forward(self,x):
    x = self.conv_layer_1(x)          # (B,C,H,W) = (B,32,256,256)
    x = self.max_pool(x)            # (B,32,128,128)
    x = self.relu(x)
    x = self.conv_layer_2(x)          # (B, 64, 128,128)
    x = self.max_pool(x)            # (B,64,64,64)
    x = self.relu(x)
    x = self.adapt_pool(x)          # (B, 64,64,64)
    x = self.flatten(x)
    x = self.fc(x)

    return x

class Double_pendulum_dataset_for_CNN:
  def __init__(self, image_tensor,ground_truth_parameters):

    self.image_tensor = image_tensor
    self.ground_truth_parameters = ground_truth_parameters

  def __len__(self):
    return self.image_tensor.shape[0]

  def __getitem__(self, idx):

    image = self.image_tensor[idx]
    ground_truth = self.ground_truth_parameters[idx]

    return image, ground_truth

full_trajectories = Double_pendulum_dataset_for_CNN(image_tensor=image_tensor, ground_truth_parameters= gt_parameters_tensor)

number_of_validation_data, number_of_testing_data = int(0.1 * len(full_trajectories)), int(0.1 * len(full_trajectories))
number_of_training_data = int(len(full_trajectories) - number_of_validation_data - number_of_testing_data)

testing_dataset, validation_dataset, training_dataset = random_split(full_trajectories, [number_of_testing_data, number_of_validation_data, number_of_training_data],torch.Generator().manual_seed(42))


training_dataloader = DataLoader(training_dataset,
                                 batch_size= 10,
                                 shuffle= True,
                                 )

validation_dataloader = DataLoader(validation_dataset,
                                   batch_size= 10,
                                   shuffle= False,
                                   )

testing_dataloader = DataLoader(testing_dataset,
                                batch_size = 10,
                                shuffle= False,
                                )

class CNN_lighting(L.LightningModule):
  def __init__(self,learning_rate):
    super().__init__()

    self.save_hyperparameters()
    self.learning_rate = learning_rate

    self.model = CNN_architecture()

    self.train_mse = MeanSquaredError()
    self.train_r2 = R2Score()

    self.valid_mse = MeanSquaredError()
    self.valid_r2 = R2Score()

    self.test_mse = MeanSquaredError()
    self.test_r2 = R2Score()

    self.counts = {"train": 0, "validation": 0, "test": 0}



  def forward(self, image_tensor):

    output = self.model(image_tensor)
    return output

  def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)

    return optimizer


  def common_step(self, batch, batch_idx,stage):
    image_tensor, ground_truth = batch

    output = self.model(image_tensor)

    loss = F.mse_loss(output, ground_truth)

    wandb.log({f"{stage}_loss": loss})
    self.log(f"{stage}_loss",loss,on_epoch=True, on_step=False,logger=True)

    if stage == "train":
        self.train_mse.update(output, ground_truth)
        self.train_r2.update(output, ground_truth)
    elif stage == "validation":
        self.valid_mse.update(output, ground_truth)
        self.valid_r2.update(output, ground_truth)
    else:
        self.test_mse.update(output, ground_truth)
        self.test_r2.update(output, ground_truth)

    # increment by batch size
    self.counts[stage] += 1



    return loss



  def training_step(self, batch, batch_idx):
    return self.common_step(batch, batch_idx, "train")

  def validation_step(self, batch, batch_idx):
    return self.common_step(batch, batch_idx, "validation")

  def test_step(self, batch, batch_idx):
    return self.common_step(batch, batch_idx, "test")

  def on_validation_epoch_end(self):

    valid_mse_result = float(self.valid_mse.compute())
    if self.counts["validation"] >= 2:
      valid_r2_result = float(self.valid_r2.compute())
    else:
      valid_r2_result = 0.0

    wandb.log({"validation_mse": valid_mse_result})
    wandb.log({"validation_r2": valid_r2_result})
    self.log("validation_mse", valid_mse_result,logger=True, prog_bar=True)
    self.log("validation_r2",  valid_r2_result,logger=True, prog_bar=True)

    self.valid_mse.reset()
    self.valid_r2.reset()


  def on_test_epoch_end(self):

    test_mse_result = float(self.test_mse.compute())
    if self.counts["test"] >= 2:
      test_r2_result = float(self.test_r2.compute())
    else:
      test_r2_result = 0.0


    wandb.log({"test_mse": test_mse_result})
    wandb.log({"test_r2": test_r2_result})
    self.log("test_mse", test_mse_result,logger=True, prog_bar=True)
    self.log("test_r2",  test_r2_result,logger=True, prog_bar=True)

    self.test_mse.reset()
    self.test_r2.reset()

  def on_train_epoch_end(self):

    train_mse_result = float(self.train_mse.compute())
    if self.counts["train"] >= 2:
      train_r2_result = float(self.train_r2.compute())
    else:
      train_r2_result = 0.0

    wandb.log({"train_mse": train_mse_result})
    wandb.log({"train_r2": train_r2_result})
    self.log("train_mse", train_mse_result,logger=True, prog_bar=True)
    self.log("train_r2",  train_r2_result,logger=True, prog_bar=True)


    self.train_mse.reset()
    self.train_r2.reset()



CNN_model = CNN_lighting(learning_rate= 3e-5)

trainer_for_CNN = L.Trainer(max_epochs= 50,
                    # fast_dev_run=True,
                    # log_every_n_steps = 1,
                    # logger = [csv_logger,tensorboard_logger, wandb_logger],
                    # callbacks=[ckpt, RichProgressBar()],
                    # default_root_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/LSTM/"
                    )
tuner = L.pytorch.tuner.Tuner(trainer_for_CNN)

lr_finder = tuner.lr_find(
    CNN_model,
    train_dataloaders= training_dataloader,
    val_dataloaders= validation_dataloader,
    min_lr=1e-6, max_lr=1e-2, num_training=100
)

fig = lr_finder.plot(suggest=True)
new_lr = lr_finder.suggestion()     # Lightning's suggestion
print("Suggested LR:", new_lr)

from lightning.pytorch.loggers import CSVLogger
from lightning.pytorch.callbacks import RichProgressBar, ModelCheckpoint
from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger, CSVLogger

tensorboard_logger = TensorBoardLogger(save_dir = "/content/drive/MyDrive/Github_projects/Double_pendulum/CNN/")
wandb_logger = WandbLogger(project="Double_Pendulum", name="CNN")
csv_logger = CSVLogger(save_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/CNN/")

ckpt = ModelCheckpoint(
    monitor="validation_mse",
    mode="min",
    save_top_k=1,
    filename="CNN-{epoch:02d}-{validation_mse:.4f}"
)

trainer_for_CNN = L.Trainer(max_epochs= 200,
                    # fast_dev_run=True,
                    log_every_n_steps = 1,
                    logger = [csv_logger,tensorboard_logger,wandb_logger],
                    accelerator="auto",
                    callbacks=[ckpt, RichProgressBar()],
                    default_root_dir="/content/drive/MyDrive/Github_projects/Double_pendulum/CNN/")

trainer_for_CNN.fit(model = CNN_model, train_dataloaders= training_dataloader, val_dataloaders= validation_dataloader)



tester = L.Trainer(
    # logger=[csv_logger, tensorboard_logger, wandb_logger],
    enable_progress_bar=False  # simplest
    # or callbacks=[]  # no RichProgressBar here
)

tester.test(
    model=CNN_model,
    dataloaders=testing_dataloader,
    ckpt_path = ""
)

import torch
import pandas as pd

def show_prediction_and_ground_truth(model, dataloader, n_limit=None):
    """
    Collect predictions and ground truths for all samples in a dataloader.

    model: trained Lightning or torch model
    dataloader: DataLoader yielding (x, y)
    param_names: list of parameter names ["L1","L2","m1","m2","k"]
    n_limit: optional cap on number of samples to include
    """
    model.eval()
    param_names = ["L1","L2","m1","m2","k"]

    preds, gts = [], []
    total = 0

    for x, y in dataloader:
        x = x.to(model.device)
        with torch.no_grad():
            y_hat = model(x)
        preds.append(y_hat.cpu())
        gts.append(y.cpu())

        total += len(y)
        if n_limit and total >= n_limit:
            break

    preds = torch.cat(preds, dim=0).numpy()
    gts = torch.cat(gts, dim=0).numpy()

    df_pred = pd.DataFrame(preds, columns=[f"pred_{n}" for n in param_names])
    df_gt = pd.DataFrame(gts, columns=[f"gt_{n}" for n in param_names])

    df = pd.concat([df_gt, df_pred], axis=1)
    return df
show_prediction_and_ground_truth(model = CNN_model, dataloader = testing_dataloader)





training_dataloader.dataset[0]

# Unpack the columns of the coords_data array into separate variables
x1, y1, x2, y2 = coords_data[:, 0], coords_data[:, 1], coords_data[:, 2], coords_data[:, 3]

plt.plot(x2,y2)

gt_parameters = pd.read_csv("/content/drive/MyDrive/Github_projects/Double_pendulum/dataset/indexes/timeseries_index.csv")

gt_parameters.iloc[1,2:-1]

# Convert the DataFrame slice to a NumPy array before creating the tensor
gt_parameters_tensor = torch.tensor(gt_parameters.iloc[:, 2:-1].values, dtype=torch.float32)
gt_parameters_tensor[1]

coords_tensors[0].shape

